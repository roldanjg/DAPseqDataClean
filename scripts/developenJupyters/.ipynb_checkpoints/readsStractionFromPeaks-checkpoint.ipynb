{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# reduce new function https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "\n",
    "commonpeaksfile = '/home/joaquin/projects/methylation/data/commonData/arabidopsisThaliana/intersect/allThePossiblePeaksnine.bed'\n",
    "\n",
    "metadata = '/home/joaquin/projects/methylation/data/commonData/ids_data_allReplicates_methylation.json'\n",
    "basePathDataFolder = '/home/joaquin/projects/methylation/data'\n",
    "narrowPeakLocationFolders = ['tfs_rep_1','tfs_rep_3_input_from_rep_2', 'tfs_rep_2', 'tfs_rep_4']\n",
    "specificPathsSumary = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata) as jsonMetadata:\n",
    "     experimentsClasification = json.load(jsonMetadata)['experiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experimentsClasification:\n",
    "    specificPathsSumary[experiment['condition']] = {}\n",
    "    for metState in ['direct', 'amplified']:\n",
    "        specificPathsSumary[experiment['condition']][metState] = {}\n",
    "        for exptype in ['sample', 'input']:\n",
    "            specificPathsSumary[experiment['condition']][metState][exptype] = {}\n",
    "            for replicate, number in zip(\n",
    "                experiment[metState],\n",
    "                ['replicate1','replicate2', 'replicate3']\n",
    "            ):\n",
    "            \n",
    "                expeId, expPath = replicate[number][0][exptype].strip().split(',')\n",
    "        # if it is a missing experiment dont continue with the analisys\n",
    "                if 'MISSING' in expeId:\n",
    "                    print(number, expPath, expeId)\n",
    "                    continue\n",
    "                for possiblenarrowPeakFolder in narrowPeakLocationFolders:\n",
    "                    narrowPeakFolder = None\n",
    "                    path = os.path.join(basePathDataFolder,possiblenarrowPeakFolder,expPath)\n",
    "        # make a list of the files in each posible directory. Try and continue if the experiment was not \n",
    "        # done for the replicate. \n",
    "                    try:\n",
    "                        filesInFolder = os.listdir(path)\n",
    "                    except FileNotFoundError:\n",
    "                        continue\n",
    "        # search for the specific experiment id inside the folder in the names to check if it is the correct folder\n",
    "        # and stop searching if it is inside\n",
    "\n",
    "                    for file in filesInFolder:\n",
    "                        if 'html' in file:\n",
    "                            fileid=file\n",
    "    #                 print(fileid)\n",
    "    #                 print(expeId)\n",
    "                    if fileid.startswith(expeId):\n",
    "                        narrowPeakFolder = possiblenarrowPeakFolder\n",
    "                        break\n",
    "\n",
    "                narrowpeakFileOriginalPath = os.path.join(\n",
    "                    basePathDataFolder,narrowPeakFolder,expPath\n",
    "                )\n",
    "    #             specificPathsSumary[experiment['condition']][metState].append((narrowPeakFolder,narrowpeakFileOriginalPath))\n",
    "                specificPathsSumary[experiment['condition']][metState][exptype][number] = narrowpeakFileOriginalPath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performIntersect(folder, intersectFile):\n",
    "    \n",
    "    sortedBamFile = glob.glob(f'{folder}/*orted.bam')\n",
    "    \n",
    "    if len(sortedBamFile) != 1:\n",
    "        return print(folder, ' has a problem selecting File')\n",
    "    else:\n",
    "        sortedBamFile = sortedBamFile[0]\n",
    "    \n",
    "    outputFile = intersectFile.strip().split('/')[-1][:-4]+'_'+sortedBamFile.split('/')[-1][:-10]+'.bed'\n",
    "    outputFilePath = os.path.join(folder,outputFile)\n",
    "\n",
    "\n",
    "    print(outputFilePath)\n",
    "    \n",
    "    if os.path.isfile(outputFilePath):\n",
    "        os.remove(outputFilePath)\n",
    "    if not os.path.isfile(outputFilePath):\n",
    "        subprocess.call(\n",
    "            'samtools' + ' view -q1 -b ' + sortedBamFile + ' | ' +\n",
    "            'bedtools' + ' intersect -abam stdin -b ' + intersectFile + ' -bed -wb -f 0.5 ' +\n",
    "            '> ' + outputFilePath , shell=True\n",
    "        )\n",
    "    else:\n",
    "        print(outputFile, ' is already done')\n",
    "    \n",
    "    totalForBox = {}\n",
    "    with open(outputFilePath, 'r') as intersectOut:\n",
    "        intersectDf = pd.read_csv(\n",
    "            intersectOut, sep='\\t', usecols=[3, 12, 13, 14, 15],\n",
    "            names=['intersected', 'chr', 'start', 'end', 'boxname'],\n",
    "        )\n",
    "        for index, ip in intersectDf.iterrows():\n",
    "            intersectOcurrence = str(ip.intersected.split('/')[0])\n",
    "            box = ','.join([str(ip.chr), str(ip.start), str(ip.end), ip.boxname])\n",
    "            if box in totalForBox:\n",
    "                totalForBox[box].add(intersectOcurrence)\n",
    "            else:\n",
    "                totalForBox[box] = {intersectOcurrence}\n",
    "\n",
    "        for box in totalForBox:\n",
    "            boxlen = len(totalForBox[box])\n",
    "            totalForBox[box] = boxlen\n",
    "\n",
    "        with open(outputFilePath[:-4] + '_boxtotals.csv', 'w') as elcsv:\n",
    "            elcsv.write('chr,start,end,boxname,{}\\n'.format(sortedBamFile.split('/')[-1][:-10]))\n",
    "            for name, recount in totalForBox.items():\n",
    "                elcsv.write('{},{}\\n'.format(name, recount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in specificPathsSumary:\n",
    "    for metState in specificPathsSumary[experiment]:\n",
    "        for exptype in specificPathsSumary[experiment][metState]:\n",
    "            for replicate in specificPathsSumary[experiment][metState][exptype]:\n",
    "                workingFolder = specificPathsSumary[experiment][metState][exptype][replicate]\n",
    "                # performIntersect(workingFolder, commonpeaksfile)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculationBowtieSummary(filepath):\n",
    "    filename = os.path.join(filepath,'bowtie2stats.txt')\n",
    "    with open(filename, 'r') as bowstats:\n",
    "        for line in bowstats:\n",
    "            reads = re.search(r'([\\d]+) reads; of these:',line)\n",
    "            regular = re.search(r'([\\d,\\.]+)% overall alignment rate',line)\n",
    "\n",
    "        return reads.group(1),regular.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateDfandNormaliceDataTPMs(folder,dataColName):\n",
    "    csvFile = glob.glob(f'{folder}/allThePossiblePeaksnine*.csv')\n",
    "    \n",
    "    if len(csvFile) != 1:\n",
    "        return print(folder, ' has a problem selecting File')\n",
    "    else:\n",
    "        csvFilePath = csvFile[0]\n",
    "\n",
    "    fileDf = pd.read_csv(csvFilePath,header=0, names=['chr','star','end','id',dataColName])\n",
    "    totalReads = fileDf[dataColName].sum()\n",
    "\n",
    "    scalingFactor = totalReads/100000\n",
    "\n",
    "    fileDf[dataColName] = fileDf[dataColName].apply(lambda x: x/scalingFactor)\n",
    "\n",
    "    return fileDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateDfandNormaliceDataRPPMs(folder,dataColName, normalizationValue):\n",
    "    \"\"\"If we want to use RPKMs we should introduce in the line before this one\n",
    "    this 2 lines:\n",
    "            reads, alingpercent = calculationBowtieSummary(workingFolder)\n",
    "            totalReads = int(round(int(reads)*(float(alingpercent)/100),0)) # ---> normalizationValue\n",
    "     \"\"\"\n",
    "    csvFile = glob.glob(f'{folder}/allThePossiblePeaksnine*.csv')\n",
    "    \n",
    "    if len(csvFile) != 1:\n",
    "        return print(folder, ' has a problem selecting File')\n",
    "    else:\n",
    "        csvFilePath = csvFile[0]\n",
    "\n",
    "    fileDf = pd.read_csv(csvFilePath,header=0, names=['chr','star','end','id',dataColName])\n",
    "    fileDf[dataColName] = fileDf[dataColName].apply(lambda x: x/scalingFactor)\n",
    "    totalReads = fileDf[dataColName].sum()\n",
    "    scalingFactor = totalReads/1000000\n",
    "    fileDf[dataColName] = fileDf[dataColName].apply(lambda x: x/scalingFactor)\n",
    "    return fileDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce new function https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "def generateMeanReplicatesDf(tf):\n",
    "    allNormalizedreplicates = []\n",
    "    for experiment in specificPathsSumary:\n",
    "        if tf in experiment:\n",
    "            for metState in specificPathsSumary[experiment]:\n",
    "                for exptype in specificPathsSumary[experiment][metState]:\n",
    "                    listofdfs = []\n",
    "                    datacolnames = []\n",
    "                    for replicate in specificPathsSumary[experiment][metState][exptype]:\n",
    "                        workingFolder = specificPathsSumary[experiment][metState][exptype][replicate]\n",
    "                        datacolname = '{}{}{}{}'.format(experiment,metState,exptype,replicate)\n",
    "                        listofdfs.append(generateDfandNormaliceDataTPMs(workingFolder,datacolname))\n",
    "                        datacolnames.append(datacolname)\n",
    "        #             los valores que no estan en una de las replicas los completo con un 0\n",
    "                    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "                                                    how='outer'), listofdfs).fillna(0)\n",
    "    #                 print(df_merged)\n",
    "                    mean_col_name = '{}{}{}'.format(experiment,metState,exptype)\n",
    "                    df_merged[mean_col_name] = df_merged[datacolnames].mean(axis=1)\n",
    "                    df_merged = df_merged.drop(columns=datacolnames)\n",
    "                    if not 'input' in mean_col_name:\n",
    "                        allNormalizedreplicates.append(df_merged)\n",
    "\n",
    "    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "                                                    how='outer'), allNormalizedreplicates).fillna(0)\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycH7df = generateMeanReplicatesDf('MYCH7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct/amplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclasification = [('MYC31Mockdirectsample', 'MYC31Mockamplifiedsample'),\n",
    " ('MYC36Mockdirectsample', 'MYC36Mockamplifiedsample'),\n",
    " ('MYC324Mockdirectsample', 'MYC324Mockamplifiedsample'),\n",
    " ('MYC31JAdirectsample', 'MYC31JAamplifiedsample'),\n",
    " ('MYC36JAdirectsample', 'MYC36JAamplifiedsample'),\n",
    " ('MYC324JAdirectsample','MYC324JAamplifiedsample'),\n",
    " ('MYC31ACCdirectsample', 'MYC31ACCamplifiedsample'),\n",
    " ('MYC36ACCdirectsample','MYC36ACCamplifiedsample'),\n",
    " ('MYC324ACCdirectsample', 'MYC324ACCamplifiedsample')\n",
    " ]\n",
    "df_plot = df_merged.drop(columns=[\"star\", \"chr\", \"end\"])\n",
    "df_plot = df_plot.set_index('id')\n",
    "df_plot = df_plot.applymap(lambda v: v+0.5)\n",
    "for (direct,amplified) in dfclasification:\n",
    "    \n",
    "    df_plot[direct[:-12]] = df_plot[direct]/df_plot[amplified]\n",
    "    df_plot = df_plot.drop(columns=[direct,amplified])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_plot.columns:\n",
    "    print(col, df_plot[col].mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MYC3\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "df_plot = df_merged.drop(columns=[\"star\", \"chr\", \"end\"])\n",
    "df_plot = df_plot.set_index('id')\n",
    "%matplotlib inline\n",
    "df_plot =df_plot.applymap(lambda v: v+0.5)\n",
    "df_plot = df_plot.applymap(lambda boxSum: math.log2(boxSum))\n",
    "img = sns.heatmap(df_plot, cmap=\"YlGnBu\", robust=True)\n",
    "# df_merged.to_csv('MYC2_meanTPM100k_log2_+0.5.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns\n",
    "CGdf = CGdf[['MYC31Mockdirectsample', 'MYC31Mockamplifiedsample',\n",
    "       'MYC31JAdirectsample', 'MYC31JAamplifiedsample', 'MYC31ACCdirectsample',\n",
    "       'MYC31ACCamplifiedsample', 'MYC36Mockdirectsample',\n",
    "       'MYC36Mockamplifiedsample', 'MYC36JAdirectsample',\n",
    "       'MYC36JAamplifiedsample', 'MYC36ACCdirectsample',\n",
    "       'MYC36ACCamplifiedsample', 'MYC324Mockdirectsample',\n",
    "       'MYC324Mockamplifiedsample', 'MYC324JAdirectsample',\n",
    "       'MYC324JAamplifiedsample', 'MYC324ACCdirectsample',\n",
    "       'MYC324ACCamplifiedsample']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "df_merged = df_merged.drop(columns=[\"star\", \"chr\", \"end\"])\n",
    "df_merged = df_merged.set_index('id')\n",
    "%matplotlib inline\n",
    "df_merged = df_merged.applymap(lambda v: v+0.01)\n",
    "df_merged = df_merged.applymap(lambda boxSum: math.log2(boxSum))\n",
    "img = sns.heatmap(df_merged, cmap=\"YlGnBu\", robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sns.heatmap(df_merged, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/\n",
    "#                 esto solo es necesario si queremos calcular los RPKMs, ahora estamos normalizando por TPMs, pero sin dividir por la \n",
    "#                 longitud del gen, porque es constante. si se quiere calcular RPKMs\n",
    "\n",
    "#         ----------------------------------------------------------------------------------\n",
    "allNormalizedreplicates = []\n",
    "for experiment in specificPathsSumary:\n",
    "    if 'MYC3' in experiment:\n",
    "        for metState in ['direct', 'amplified']:\n",
    "            listofdfNormalizedReplicates = []\n",
    "            NormalizedReplicatesnames = []\n",
    "            for replicate in ['replicate1','replicate2', 'replicate3']:\n",
    "                skiptReplicate = False\n",
    "                sampleAndControl = []\n",
    "                sampleAndControlNames = []\n",
    "                for exptype in ['sample', 'input']:\n",
    "                    try:\n",
    "                        workingFolder = specificPathsSumary[experiment][metState][exptype][replicate]\n",
    "                    except KeyError:\n",
    "                        skiptReplicate = True\n",
    "                        break\n",
    "                    datacolname = '{}{}{}{}'.format(experiment,metState,exptype,replicate)\n",
    "                    sampleAndControlNames.append(datacolname)\n",
    "                    sampleAndControl.append(generateDfandNormaliceDataTPMs(workingFolder,datacolname))\n",
    "\n",
    "                if not skiptReplicate:\n",
    "                    df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "                                                    how='outer'), sampleAndControl).fillna(0)\n",
    "                    ratio_col_name = '{}{}{}'.format(experiment,metState,replicate)\n",
    "                    df_merged[ratio_col_name] = df_merged[sampleAndControlNames[0]]/df_merged[sampleAndControlNames[1]]\n",
    "                                                                                    \n",
    "                    df_merged = df_merged.drop(columns=sampleAndControlNames)\n",
    "                    listofdfNormalizedReplicates.append(df_merged)\n",
    "                    NormalizedReplicatesnames.append(ratio_col_name)\n",
    "\n",
    "            df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "                                                how='outer'), listofdfNormalizedReplicates).fillna(0)\n",
    "            mean_col_name = '{}{}'.format(experiment,metState)\n",
    "            df_merged[mean_col_name] = df_merged[NormalizedReplicatesnames].mean(axis=1)\n",
    "            df_merged = df_merged.drop(columns=NormalizedReplicatesnames)\n",
    "            allNormalizedreplicates.append(df_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b311b5e2c1aaa04d48066871c65b4e8ce80fb760bb38a0437727056581097150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
