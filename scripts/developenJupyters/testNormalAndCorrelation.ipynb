{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicate3 MYC3/amplified/6/Mock MISSING\n",
      "replicate2 MYCH7/direct/24/Mock MISSING\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "# reduce new function https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "\n",
    "commonpeaksfile = '/home/joaquin/projects/methylation/data/commonData/arabidopsisThaliana/intersect/allThePossiblePeaksnine.bed'\n",
    "\n",
    "metadata = '/home/joaquin/projects/methylation/data/commonData/ids_data_allReplicates_methylation.json'\n",
    "basePathDataFolder = '/home/joaquin/projects/methylation/data'\n",
    "narrowPeakLocationFolders = ['tfs_rep_1','tfs_rep_3_input_from_rep_2', 'tfs_rep_2', 'tfs_rep_4']\n",
    "specificPathsSumary = {}\n",
    "with open(metadata) as jsonMetadata:\n",
    "     experimentsClasification = json.load(jsonMetadata)['experiments']\n",
    "for experiment in experimentsClasification:\n",
    "    specificPathsSumary[experiment['condition']] = {}\n",
    "    for metState in ['direct', 'amplified']:\n",
    "        specificPathsSumary[experiment['condition']][metState] = {}\n",
    "        for exptype in ['sample', 'input']:\n",
    "            specificPathsSumary[experiment['condition']][metState][exptype] = {}\n",
    "            for replicate, number in zip(\n",
    "                experiment[metState],\n",
    "                ['replicate1','replicate2', 'replicate3']\n",
    "            ):\n",
    "            \n",
    "                expeId, expPath = replicate[number][0][exptype].strip().split(',')\n",
    "        # if it is a missing experiment dont continue with the analisys\n",
    "                if 'MISSING' in expeId:\n",
    "                    print(number, expPath, expeId)\n",
    "                    continue\n",
    "                for possiblenarrowPeakFolder in narrowPeakLocationFolders:\n",
    "                    narrowPeakFolder = None\n",
    "                    path = os.path.join(basePathDataFolder,possiblenarrowPeakFolder,expPath)\n",
    "        # make a list of the files in each posible directory. Try and continue if the experiment was not \n",
    "        # done for the replicate. \n",
    "                    try:\n",
    "                        filesInFolder = os.listdir(path)\n",
    "                    except FileNotFoundError:\n",
    "                        continue\n",
    "        # search for the specific experiment id inside the folder in the names to check if it is the correct folder\n",
    "        # and stop searching if it is inside\n",
    "\n",
    "                    for file in filesInFolder:\n",
    "                        if 'html' in file:\n",
    "                            fileid=file\n",
    "    #                 print(fileid)\n",
    "    #                 print(expeId)\n",
    "                    if fileid.startswith(expeId):\n",
    "                        narrowPeakFolder = possiblenarrowPeakFolder\n",
    "                        break\n",
    "\n",
    "                narrowpeakFileOriginalPath = os.path.join(\n",
    "                    basePathDataFolder,narrowPeakFolder,expPath\n",
    "                )\n",
    "    #             specificPathsSumary[experiment['condition']][metState].append((narrowPeakFolder,narrowpeakFileOriginalPath))\n",
    "                specificPathsSumary[experiment['condition']][metState][exptype][number] = narrowpeakFileOriginalPath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performShapiroTest(dfcol): #ej phenotypeValuesDf['notTransformed']\n",
    "    # THE DATA IN ARRAY\n",
    "#     phenotypeValuesDf = pd.DataFrame.from_dict(phenotypeValuesDict, orient='index', columns=['notTransformed'])\n",
    "#     example for testing {'a': -2, 'b': 1, 'c': 3, 'd': 1, 'e': 2, 'f': 8}\n",
    "    shapiro_test = stats.shapiro(dfcol)\n",
    "    return shapiro_test.pvalue\n",
    "\n",
    "def performPearsonCorr(datasetone,datasettwo):\n",
    "    return stats.pearsonr(datasetone,datasettwo)\n",
    "\n",
    "def generateDfandNormaliceDataTPMs(folder,dataColName):\n",
    "    csvFile = glob.glob(f'{folder}/allThePossiblePeaksnine*.csv')\n",
    "    \n",
    "    if len(csvFile) != 1:\n",
    "        return print(folder, ' has a problem selecting File')\n",
    "    else:\n",
    "        csvFilePath = csvFile[0]\n",
    "\n",
    "    fileDf = pd.read_csv(csvFilePath,header=0, names=['chr','star','end','id',dataColName])\n",
    "    totalReads = fileDf[dataColName].sum()\n",
    "\n",
    "    scalingFactor = totalReads/100000\n",
    "\n",
    "    fileDf[dataColName] = fileDf[dataColName].apply(lambda x: x/scalingFactor)\n",
    "\n",
    "    return fileDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce new function https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes\n",
    "import numpy as np\n",
    "with open('datacorrgoodTPM100K.tsv', 'a+') as datacorr:\n",
    "    for experiment in specificPathsSumary:\n",
    "        for metState in specificPathsSumary[experiment]:\n",
    "            for exptype in specificPathsSumary[experiment][metState]:\n",
    "                listofdfs = []\n",
    "                datacolnames = []\n",
    "                for replicate in specificPathsSumary[experiment][metState][exptype]:                    \n",
    "                    workingFolder = specificPathsSumary[experiment][metState][exptype][replicate]\n",
    "                    datacolname = '{}{}{}{}'.format(experiment,metState,exptype,replicate)\n",
    "                    replicateTPMdf = generateDfandNormaliceDataTPMs(workingFolder,datacolname)\n",
    "#                     print(f'{datacolname} shapiro test is ', performShapiroTest(np.array(replicateTPMdf[datacolname])))\n",
    "                    listofdfs.append(replicateTPMdf)\n",
    "                    datacolnames.append(datacolname)\n",
    "                    \n",
    "\n",
    "    #             los valores que no estan en una de las replicas los completo con un 0\n",
    "                df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "                                                how='outer'), listofdfs).fillna(0)\n",
    "                mean_col_name = '{}_{}_{}'.format(experiment,metState,exptype)\n",
    "#                 df_merged[mean_col_name] = df_merged[datacolnames].mean(axis=1)\n",
    "                listOfSubsets = []\n",
    "                listOfSubsets.append(mean_col_name)\n",
    "                for subset in itertools.combinations(datacolnames, 2):\n",
    "                    corr, _ = performPearsonCorr(np.array(df_merged[subset[0]]), np.array(df_merged[subset[1]]))\n",
    "                    listOfSubsets.append(f'{subset[0][-10:]}_vs_{subset[1][-10:]}:'+str(corr))\n",
    "                datacorr.write('{}\\n'.format('\\t'.join(listOfSubsets)))\n",
    "            #                 df_merged = df_merged.drop(columns=datacolnames)\n",
    "                \n",
    "#                 if not 'input' in mean_col_name:\n",
    "#                     allNormalizedreplicates.append(df_merged)\n",
    "\n",
    "# df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['chr','star','end','id'],\n",
    "#                                                 how='outer'), allNormalizedreplicates).fillna(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro \n",
    "from scipy.stats import lognorm\n",
    "\n",
    "#make this example reproducible\n",
    "np.random.seed(1)\n",
    "\n",
    "#generate dataset that contains 1000 log-normal distributed values\n",
    "lognorm_dataset = lognorm.rvs(s=.5, scale=math.exp(1), size=8000)\n",
    "shapiro(lognorm_dataset)\n",
    "\n",
    "# ShapiroResult(statistic=0.8573324680328369, pvalue=3.880663073872444e-29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lognorm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in itertools.combinations(specificPathsSumary[experimentCondition][metstate], 2):\n",
    "            endProcess = subprocess.run(\n",
    "                'bedtools intersect -wb -wa -a {} -b {} -f 0.6 -r'.format(subset[0], subset[1]),\n",
    "                shell=True,\n",
    "                capture_output=True\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
